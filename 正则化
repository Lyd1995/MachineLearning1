一、过拟合
产生原因：特征很多但是样本数量很少

解决方法：
1、减少特征数量
（1）人工选择特征
（2）建立模型，自动选择

2、正则化
保留所有的特征，但是减小量级或者减小参数的值
这个方法很有效，当有许多特征时

二、代价函数
欧氏距离：（也是均方差）

在线性回归中，一般使用均方差作为代价函数

修改一下代价函数，在其中加入其它项------高阶参数thetan（n很大，m <= n <= max），并且给这些theta以一个非常大的系数
这里，新加入的项被称为惩罚项

正则化的思想：  通过减小theta的值，使得整个模型得以简化
在房子价格预测的例子中可以看出，当theta3和theta4都很小很小时，预测函数实际上就从四次函数变为了一个二次函数

实际中使用正则化并不简单，因为你无法知道在这众多特征中，哪个特征的相关度高，哪个特征的相关度低
既然如此，那就给所有的参数theta施加惩罚（一般来说常数项theta0不会施加惩罚），这就是之前说的，减小参数的值

lamda：正则化参数，用来控制参数theta的大小
如果lambda太大，施加的惩罚就会很大，拟合曲线会趋于一条直线

正则化参数一般由算法自动选择


三、线性回归的正则化

1、梯度下降法更新
（1）使用加入参数theta之后的代价函数
（2）theta0分离出来单独更新（因为theta0上没有施加惩罚）
（3）同时更新theta1~thetan

2、正规方程
修改为：XTX+lambda*伪单位矩阵（就是第一行全为0）即可


四、逻辑回归的正则化

1、梯度下降法
表示形式与现行回归中的很像，但是它们的假设模型不同

五、神经网络（非线性）

引例：房屋预测，根据房屋的特征，预测半年内可以被卖出去的概率
显然这是一个非线性的模型（事实上，现实生活中的大部分预测都是非线性的）

（1）有两项特征，可以将特征的不同次幂进行组合，得到一个高阶多项式（非线性模型）

（2）但是当房屋有100项特征时，再采用这个方法，恐怕就行不通了，因为光是一次项就有100个，二次项的个数为：100+C(100,2)=5050
三次项的个数约为：170000
可以看出，高次项使得特征空间急剧膨胀，计算量也随之急速增加
