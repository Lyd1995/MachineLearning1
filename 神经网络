一、神经网络（非线性）

1、引例1：房屋预测，根据房屋的特征，预测半年内可以被卖出去的概率
显然这是一个非线性的模型（事实上，现实生活中的大部分预测都是非线性的）

（1）有两项特征，可以将特征的不同次幂进行组合，得到一个高阶多项式（非线性模型）

（2）但是当房屋有100项特征时，再采用这个方法，恐怕就行不通了，因为光是一次项就有100个，二次项的个数为：100+C(100,2)=5050
三次项的个数约为：170000
可以看出，高次项使得特征空间急剧膨胀，计算量也随之急速增加

2、引例2：计算机视觉问题，判别图像是否为汽车

为什么计算机识别一张图像很困难？
因为在计算机眼中，图像仅仅是一个由亮度像素组成的矩阵，里面存放了大量的数值
显然，对于识别图像来说，矩阵中的每一个数据都是有用的，因此，一副1920*1080分辨率的图像，有2073600个特征（灰度图片，彩色还要*3）
如果按照之前的逻辑回归去分类，几乎是不可能的

*******************逻辑回归、线性回归算法适用于特征较少的情况

对于复杂的非线性假设模型，神经网络算法被证明是一种要好得多的算法，即便输入的特征空间很大

3、神经网络的起源-----大脑

实际上，将动物的神经网络进行剪切并重定向，可以让眼睛学会听，耳朵学会看
等等许多例子，事实上，可以将许多传感器移植到大脑，大脑就能学会如何处理这些数据

4、神经元
神经元有很多输入，一个输出，它将输入的信息进行计算最后输出到下一个神经元

神经网络使用逻辑元来模拟神经元

5、神经网络中的术语

权重weight   对应于   参数theta

输入层：第一层
输出层：最后一层
隐藏层：中间的，也有说法叫：计算层

神经网络的结构：指的是神经元的连接方式

神经元模型决定g(x)

二、深入理解神经网络的工作原理

1、神经网络为什么有效？
假设激活函数和逻辑回归的假设模型一致
首先，如果仅有一层的神经网络，那么它实际上就是一个逻辑回归模型

事实上，对多层（假设n层）的神经网络也可以看成是一个逻辑回归模型，从输出层（第n层）来看，它的输入是上一层（第n-1层）的输出
第一层的输入为：x1、x2...xm
第n-1层的输出为：a1、a2...ak

逻辑回归是将第一层的输入经过计算然后输出得到结果
而神经网络是将第一层的输入经过计算变换为其他的特征，用得到新的特征进行计算预测，它没有直接用现成的特征，而是通过训练得到新的特征

2、引例1：建立一个能够拟合非异或（XNOR）运算的神经网络

首先，从建立一个能够拟合与（AND）运算的神经网络
h(x):采用阶跃函数或者S函数      输入为：x1、x2、1（偏置）      参数：-30,20,20
因此：h(x)=g(-30+20*x1+20*x2)     这样可以近似得到与操作

如何建立或（OR）运算的神经网络：
修改参数即可：-10,20,20

如何建立非（NOT）运算的神经网络：
修改参数：10（1），-20（x）

建立非异或（XNOR）神经网络：
首先搭建：
（1）与运算的神经网络（x1 AND x2），输出为a1
（2）(NOT x1)  AND  (NOT x2)，输出为a2
（3）将上面两个神经网络的输出，使用或操作： a1 OR a2
得到了一个非线性的决策边界

3、神经网络解决多元分类问题
本质上是：一对多方法的拓展

例如：对图像进行分类：行人、汽车、摩托、卡车
首先：建立一个含有四个输出神经元的神经网络
输入：图片
输出：一个一维向量，输出是四个逻辑回归分类器
以前使用一个数来代表输出类型，现在改用向量

三、代价函数

1、引例
（1）二分类问题：k=2（k是分类种数）
输出：实数y   并且取值只能是0或者1
最后一层只有一个神经元

（2）多分类问题：k >= 3（k是分类种数）
对图像进行分类：行人、汽车、摩托、卡车
首先：建立一个含有四个输出神经元的神经网络
输入：图片

2、同样地需要对其进行正则化，加入参数

输出：一维向量y，输出是四个逻辑回归分类器
以前使用一个数来代表输出类型，现在改用向量

y=[y1,y2,y3,y4]   这里yn的取值只能是： 0 或者 1

四、反向传播

前向传播：输入---->输出    沿着这个方向前进，一步一步得到输出，就被称为前向传播

反向传播：
首先需要计算误差
设输出为al，训练集给出的标准为y
计算输出层误差errorl=al-y
再根据输出层误差，反向计算第l-1层的误差：(errorl-1)=theta3*errorl.*(g(z)的偏导数)
